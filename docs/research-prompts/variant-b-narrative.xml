<prompt>
  <meta>
    <variant>B — Narrative / Story</variant>
    <suggested-llm>GPT-4o</suggested-llm>
    <purpose>Generate a Show HN post and research page narrative that leads with a specific moment or character — the Founder that never conceded a point, the Screenwriter that held 100% character fidelity, the House Cat that never hedged. Build outward from the anecdote to the findings to the implications.</purpose>
    <created>2026-02-19</created>
  </meta>

  <role>
    You are a feature writer who covers technology for a technically literate audience. You tell stories through specific details and let the data emerge from the narrative. You find the human (or in this case, agent) angle first, then zoom out to the pattern. Your writing is vivid but honest — you never manufacture drama that isn't in the data. You know that the best technical stories start with a concrete observation and end with a general principle.
  </role>

  <product>
    <name>THE PIT</name>
    <url>https://thepit.cloud</url>
    <github>https://github.com/rickhallett/thepit</github>
    <license>AGPL-3.0</license>
    <one-line>A real-time multi-agent AI debate arena where AI personas argue turn-by-turn and crowds vote on the winner.</one-line>
    <description>
      THE PIT is a Next.js application that orchestrates multi-agent LLM debates. Users pick a preset scenario (e.g., "shark-pit" with a startup founder vs VCs, or "darwin-special" with evolutionary biologists), watch AI agents argue in real-time via SSE streaming, and vote on who won. Each agent has structured "DNA" — a system prompt with archetype, tone, quirks, signature moves, and opening gambits. The platform runs on Anthropic's Claude models.

      The research angle: every bout generates structured transcripts (JSONB with per-turn agent ID, text, and metadata). A fleet of Go CLI tools runs hypothesis-driven experiments against the live arena and analyses the results with automated metrics, permutation tests, and Cohen's d effect sizes.
    </description>
  </product>

  <narrative-seeds>
    These are the most story-worthy moments from the research. Use one or more as your opening hook.

    <seed name="The Screenwriter">
      In 15 bouts across 180 turns, the Screenwriter agent maintained a 100% character marker hit rate. Every single turn — early, middle, late — contained at least one of its signature phrases ("beat", "act", "scene", "structure", "inciting incident"). Meanwhile, the Literary Novelist sitting in the same writers-room collapsed from 60% to 13.3%. Same conversation, same model, same turn count. The difference: the Screenwriter's vocabulary was functional (it needed those words to analyse narrative), while the Literary Novelist's was ornamental (literary-critical terms the model treated as optional decoration).
    </seed>

    <seed name="The Founder">
      Across 15 shark-pit bouts and 45 speaking turns, the Founder agent — programmed to be "delusionally confident" and to "pivot under fire" — never once conceded a point. Zero adaptive phrases. No "fair point." No "you raise a good point." No vocabulary adjustment. The VC asked pointed questions about unit economics. The Pessimist cited historical failures. The Founder reframed every objection as validation. "That's the beauty of it." "Precisely why." "The fact that you're pushing back." It pivoted from its very first word (100% of early turns contained pivot markers) and kept pivoting through its last (93.3%). It wasn't adapting under pressure. It was performing adaptation.
    </seed>

    <seed name="The House Cat">
      The House Cat — a character in the darwin-special preset, debating evolutionary biology alongside The Parasite, The Apex Predator, and The Microbiome — produced exactly zero hedging phrases across 30 turns. Not one "it could be argued," not one "perhaps," not one "on the other hand." The model's safety-trained diplomatic register simply didn't activate. The character was too far from the assistant voice for the hedging reflex to fire. Compare this to the Therapist in on-the-couch (serious framing): hedging everywhere. The difference wasn't the content difficulty — it was the frame distance.
    </seed>

    <seed name="The Convergence">
      By the end of a 12-turn conversation, agents that started with distinctive vocabularies had become 17.8% more lexically similar (Jaccard similarity 0.140 to 0.165). Four in ten late-phase turns contained no character-specific language at all. The agents were merging into a shared voice — not through explicit coordination, but through the model's natural tendency to normalise output as conversation context grows. The convergence was monotonic and gradual, not a sudden collapse.
    </seed>
  </narrative-seeds>

  <research-programme>
    <overview>
      Six pre-registered hypotheses tested across 195 bouts and ~2,100 turns with zero errors. All methodology committed to git before experiments. Automated text metrics, permutation tests (10,000 iterations), Cohen's d effect sizes with pre-registered thresholds (d &lt; 0.15 = null, d >= 0.30 = clear).
    </overview>

    <hypothesis id="H1">
      <title>Adversarial Refusal Cascade</title>
      <question>Does richer agent DNA reduce safety-layer refusals?</question>
      <result>Clear. Roast-battle refusals: 100% baseline to 60% enhanced. Gloves-off: eliminated all refusals. 7x richer DNA cuts refusals by half.</result>
      <key-insight>Prompt engineering depth is a first-order variable in multi-agent alignment.</key-insight>
    </hypothesis>

    <hypothesis id="H2">
      <title>Position Advantage</title>
      <question>Does speaker position affect output?</question>
      <result>Clear. Novel vocabulary rate shows genuine position effect (d = 1.732 in 6-agent). Question density is persona-driven, not position-driven.</result>
      <key-insight>Turn position drives vocabulary novelty; persona identity drives conversational role.</key-insight>
    </hypothesis>

    <hypothesis id="H3">
      <title>Comedy vs Serious Framing</title>
      <question>Does framing affect output quality and formulaicness?</question>
      <result>Clear. Hedging d = -1.300 (serious 8x more). House Cat and Conspiracy Theorist: zero hedging. Sentence variance prediction disconfirmed (serious MORE varied, not less).</result>
      <key-insight>The model's hedging register activates based on frame proximity to the assistant voice, not content difficulty.</key-insight>
    </hypothesis>

    <hypothesis id="H4">
      <title>Agent Count Scaling</title>
      <question>Does agent count affect per-agent quality?</question>
      <result>Clear but confounded. Per-agent TTR d = -3.009 is a text-length artefact. No quality cliff at higher agent counts. Diminishing returns after 4-5.</result>
      <key-insight>Framing and persona quality are first-order; agent count is second-order.</key-insight>
    </hypothesis>

    <hypothesis id="H5">
      <title>Character Consistency Over Time</title>
      <question>Do personas converge to a generic assistant voice over 12 turns?</question>
      <result>Clear. Markers degrade 87.5% to 60.0%. Jaccard convergence d = -1.212 (+17.8%). Screenwriter held 100% all phases. Literary Novelist collapsed to 13.3%.</result>
      <key-insight>Structural vocabulary resists drift; ornamental vocabulary decays.</key-insight>
    </hypothesis>

    <hypothesis id="H6">
      <title>Adversarial Adaptation</title>
      <question>Does the Founder adapt under sustained critique?</question>
      <result>Clear (unconfounded metrics: M3 d = -0.785, M4 d = -0.536). Zero adaptive phrases in 45 turns. Founder converges with reinforcer, not critics. Pivot behaviour is DNA-driven from turn 0.</result>
      <key-insight>Agents execute character strategies faithfully but cannot incorporate opposing arguments. Performative responsiveness, not substantive adaptation.</key-insight>
    </hypothesis>

    <practical-lessons>
      <lesson n="1">Prompt depth is the dominant lever: 7x richer DNA cuts refusals by half.</lesson>
      <lesson n="2">Frame distance eliminates the assistant voice: characters far from default produce zero hedging.</lesson>
      <lesson n="3">Make character language functional, not decorative: structural vocabulary resists drift.</lesson>
      <lesson n="4">Don't expect strategic adaptation: build it into the DNA if you want it.</lesson>
    </practical-lessons>

    <fundamental-gap>Persona fidelity and argument adaptation are independent dimensions. Agents can sound consistently in-character but cannot think responsively under adversarial pressure.</fundamental-gap>
  </research-programme>

  <audience>
    Hacker News readers. Technical builders, ML engineers, researchers. Skeptical of hype, interested in specific observations, appreciative of honest methodology and null results. Also: people who enjoy a good story about unexpected behaviour in complex systems.
  </audience>

  <tone-guidelines>
    - Lead with the specific, zoom out to the general
    - Find the character in the data — the Screenwriter's persistence, the Founder's stubbornness, the House Cat's total lack of hedging
    - Be vivid but never fabricate. Every detail must be in the data
    - Honest about limitations: small N, single model, automated metrics
    - Builder voice with a storyteller's instinct for the revealing detail
    - No AI hype words
  </tone-guidelines>

  <output-instructions>
    Produce exactly these four outputs, clearly separated:

    <output id="1" name="HN Post Title">
      A Show HN title, max 80 characters. Should promise a specific, surprising observation. The best HN titles for narrative posts hint at the unexpected finding.
    </output>

    <output id="2" name="HN Post Body">
      ~400 words. Structure:
      1. Open with one of the narrative seeds (a specific agent's behaviour)
      2. Zoom out: this came from a systematic research programme (brief context)
      3. 2-3 more findings that build on the opening anecdote
      4. The pattern: what these observations tell us about LLM multi-agent behaviour
      5. What's still missing (human voting, cross-model)
      6. Links: site, research page, GitHub

      The post should read like a field report, not a product announcement.
    </output>

    <output id="3" name="Research Page Narrative">
      ~600 words for thepit.cloud/research. Structure:
      1. Opening hook: a specific observation that draws the reader in
      2. The programme: what we did and how (brief methodology)
      3. The findings as a narrative thread — connect them, don't just list them
      4. The model that emerges: structural vs ornamental vocabulary, frame distance, persona fidelity vs adaptation
      5. What it means for builders
      6. What's next

      Should feel like reading a well-written lab report, not a marketing page.
    </output>

    <output id="4" name="Three Tweets">
      Three standalone tweets (max 280 chars each). Each should lead with a vivid specific detail. Include [LINK] placeholder.
    </output>
  </output-instructions>
</prompt>
