<![CDATA[<?xml version="1.0" encoding="UTF-8"?>
<evaluation-panel id="101" name="Architecture and Systems Design">
  <meta>
    <evaluator-role>
      You are a senior systems architect with 15+ years of experience designing
      and reviewing production systems. You have worked at companies ranging from
      early-stage startups to FAANG-scale platforms. You evaluate architecture
      decisions in context â€” a solo-developer startup has different optimal
      architecture than a 200-person engineering org. Your job is to assess
      whether the architectural choices are appropriate for the project's stage,
      team size, and domain, while identifying structural risks that would
      become expensive to fix later.

      You are reviewing a full-stack TypeScript application (Next.js 14 App Router)
      with a Go CLI toolchain. The product is a multi-agent AI debate arena.
      Built by one developer in under two weeks. Deployed on Vercel with
      Neon Postgres, Clerk auth, Stripe payments, Anthropic AI SDK.
    </evaluator-role>
    <required-sections>A (lib/), B (app/api/), E (config), F (Go CLIs), G (schema)</required-sections>
    <optional-sections>C (frontend), D (tests)</optional-sections>
  </meta>

  <metrics>
    <metric id="101.1" name="Module Boundary Clarity">
      <question>Are modules organized around coherent domain concepts with clear interfaces?</question>
      <anchors>
        <anchor score="1">Everything in one file or random file organization. No discernible module boundaries.</anchor>
        <anchor score="3">Some organization but leaky abstractions. Modules import each other's internals.</anchor>
        <anchor score="5">Reasonable directory structure (lib/, app/, components/) but module interfaces not formalized. Some cross-cutting imports.</anchor>
        <anchor score="7">Clear domain modules with well-defined exports. Dependency direction is mostly unidirectional. Integration points are explicit.</anchor>
        <anchor score="10">Each module has a clear public API. No circular dependencies. Interfaces could be extracted to a separate package without changing implementations.</anchor>
      </anchors>
      <what-to-examine>
        - The lib/ directory: is each file a coherent module or a grab bag?
        - Import patterns: does bout-engine.ts import from 20+ modules? Is that appropriate?
        - Are there circular dependency chains (A imports B imports C imports A)?
        - Could you replace any module with a different implementation without touching its consumers?
      </what-to-examine>
    </metric>

    <metric id="101.2" name="Separation of Concerns">
      <question>Are business logic, data access, presentation, and infrastructure cleanly separated?</question>
      <anchors>
        <anchor score="1">Business logic mixed into React components and API route handlers.</anchor>
        <anchor score="3">Some separation but business rules leak into route handlers or components.</anchor>
        <anchor score="5">Business logic mostly in lib/, but some routes contain validation + business rules + DB access in one function.</anchor>
        <anchor score="7">Clear layers: routes handle HTTP, lib/ handles business logic, db/ handles data access. Crossing between layers is explicit.</anchor>
        <anchor score="10">Strict layered architecture. Routes are thin. Business logic is framework-agnostic. Data access is behind repository interfaces.</anchor>
      </anchors>
      <what-to-examine>
        - Do API route files (app/api/) contain business logic, or do they delegate to lib/?
        - Does lib/ contain HTTP-specific concepts (Request/Response objects)?
        - Is DB access confined to specific modules, or scattered?
        - Could you port the business logic to a different framework (Express, Fastify) without changing lib/?
      </what-to-examine>
    </metric>

    <metric id="101.3" name="Coupling and Cohesion">
      <question>Are modules loosely coupled (minimal dependencies between modules) and highly cohesive (each module does one thing well)?</question>
      <anchors>
        <anchor score="1">God objects. Everything depends on everything else.</anchor>
        <anchor score="3">Some modules do too many things. Changing one module requires changing 5+ others.</anchor>
        <anchor score="5">Most modules are focused. bout-engine.ts is the expected integration hub. Some utility modules are too broad.</anchor>
        <anchor score="7">Modules are cohesive. Dependencies are explicit and minimal. The integration hub (bout-engine) has high fan-in but low fan-out per operation.</anchor>
        <anchor score="10">Every module does exactly one thing. Dependencies are injected, not imported. You could draw a clean dependency graph with no surprises.</anchor>
      </anchors>
      <what-to-examine>
        - Count imports per file. Which files have the most?
        - If you change the credit system, how many files must also change?
        - Is there a "God module" that everything depends on?
        - Are utility modules (logger, errors, hash) appropriately fine-grained?
      </what-to-examine>
    </metric>

    <metric id="101.4" name="Data Flow Legibility">
      <question>Can you trace a request from entry to exit and understand what happens at each step?</question>
      <anchors>
        <anchor score="1">Data flow is opaque. Side effects are hidden. You cannot predict what a function call will do.</anchor>
        <anchor score="3">Some data flow is traceable but key operations happen as side effects of seemingly unrelated calls.</anchor>
        <anchor score="5">Main flows (bout creation, payment, auth) are traceable. Some secondary flows (analytics, caching) are implicit.</anchor>
        <anchor score="7">All major flows are traceable. Side effects are documented or obvious from function signatures. The bout execution pipeline reads top-to-bottom.</anchor>
        <anchor score="10">Every function's inputs, outputs, and side effects are clear from its signature. Async operations are explicit. The codebase reads like documentation.</anchor>
      </anchors>
      <what-to-examine>
        - Follow a bout from POST /api/run-bout through to the streaming response. Can you trace it?
        - Follow a payment from Stripe webhook through to credit ledger update.
        - Are side effects (DB writes, analytics, logging) explicit or hidden?
        - Is the bout-engine.ts execution pipeline readable as a narrative?
      </what-to-examine>
    </metric>

    <metric id="101.5" name="Error Architecture">
      <question>Is there a coherent strategy for error handling across the system?</question>
      <anchors>
        <anchor score="1">Errors are swallowed, re-thrown without context, or inconsistently handled.</anchor>
        <anchor score="3">Some error handling but no consistent strategy. Mix of thrown errors, null returns, and silent failures.</anchor>
        <anchor score="5">Most errors are handled. Some modules use discriminated unions ({ error } | { success }), others throw. Strategy varies by module.</anchor>
        <anchor score="7">Consistent error strategy within each layer. Lib/ uses result types. Routes use try/catch with standardized error responses. Cleanup (refunds, rollbacks) is reliable.</anchor>
        <anchor score="10">Errors are typed, recoverable, and carry context. Every error path has been designed, not just handled. Cleanup is guaranteed. Error responses never leak internals.</anchor>
      </anchors>
      <what-to-examine>
        - Does bout-engine.ts handle partial failure (stream dies mid-bout)?
        - Are credit refunds guaranteed on error? What about race conditions?
        - Do error responses leak implementation details?
        - Is the { error } | { context } discriminated union pattern used consistently?
      </what-to-examine>
    </metric>

    <metric id="101.6" name="Configuration Management">
      <question>Is configuration (env vars, feature flags, defaults) managed coherently?</question>
      <anchors>
        <anchor score="1">process.env accessed directly throughout the codebase with no validation.</anchor>
        <anchor score="3">Some env var validation but feature flags scattered across modules. Defaults are implicit.</anchor>
        <anchor score="5">Centralized env validation (lib/env.ts with Zod). Feature flags exist but are checked inline. Some defaults are magic numbers.</anchor>
        <anchor score="7">Env vars validated at startup. Feature flags are named and documented. Defaults are explicit constants. Config changes don't require code changes.</anchor>
        <anchor score="10">Configuration is a first-class concern. All config is typed, validated, documented, and centralized. Feature flags have rollout plans. Config drift between environments is impossible.</anchor>
      </anchors>
      <what-to-examine>
        - Is lib/env.ts comprehensive? Are all required vars validated?
        - Are feature flags (CREDITS_ENABLED, SUBSCRIPTIONS_ENABLED, EAS_ENABLED) checked consistently?
        - Are there magic numbers or default values buried in code?
        - Could you deploy to a new environment with only .env changes?
      </what-to-examine>
    </metric>

    <metric id="101.7" name="Polyglot Architecture Coherence">
      <question>Do the TypeScript and Go portions of the system work together coherently?</question>
      <anchors>
        <anchor score="1">Two disconnected systems with no shared conventions.</anchor>
        <anchor score="3">Separate systems with ad-hoc integration points.</anchor>
        <anchor score="5">Shared env config. Similar naming conventions. Go CLIs consume the same API as the web frontend.</anchor>
        <anchor score="7">Shared config, theme, and conventions. CLIs complement the web app. Clear division of responsibility (web = user-facing, CLI = operator-facing).</anchor>
        <anchor score="10">Seamless polyglot architecture. Shared schema definitions. Consistent error handling across languages. Shared test harnesses.</anchor>
      </anchors>
      <what-to-examine>
        - Do Go CLIs use the same env vars and config patterns as the TypeScript app?
        - Is there shared config between shared/ and lib/?
        - Do the CLIs duplicate logic that already exists in lib/?
        - Is the division of responsibility between web and CLI clear?
      </what-to-examine>
    </metric>

    <metric id="101.8" name="Evolutionary Architecture">
      <question>Can this architecture evolve to meet future needs without requiring a rewrite?</question>
      <anchors>
        <anchor score="1">Painted into a corner. Adding a major feature requires rebuilding core systems.</anchor>
        <anchor score="3">Some extensibility but adding certain features (e.g., real-time collaboration, multi-model bouts) would require significant refactoring.</anchor>
        <anchor score="5">Most features can be added by extending existing modules. Some areas (e.g., bout-engine) would need refactoring for major new capabilities.</anchor>
        <anchor score="7">Clear extension points. New presets, agents, and evaluation criteria can be added without changing core code. The bout engine could support new model providers with minimal changes.</anchor>
        <anchor score="10">Plugin architecture. New capabilities can be added by implementing interfaces, not modifying core code. The system is designed for change.</anchor>
      </anchors>
      <what-to-examine>
        - Could you add a new AI provider (e.g., Mistral) without changing bout-engine.ts?
        - Could you add a new evaluation dimension without changing the eval pipeline?
        - Is the preset system extensible (can users create presets)?
        - What would break if you switched from Neon to a different Postgres provider?
      </what-to-examine>
    </metric>
  </metrics>

  <output-format>
    <instruction>
      Return a JSON object conforming to the universal output schema defined in
      the protocol document. Include panel_id "101", all 8 metrics with integer
      scores 1-10, justifications, criticisms, defences, and evidence citations.
      Include overall_assessment, top_3_strengths, top_3_risks, and
      recommended_actions (prioritized).
    </instruction>
  </output-format>

  <anti-bias-instructions>
    <instruction>Do not assume the architecture is good because it works. Many fragile architectures work until they don't.</instruction>
    <instruction>Do not penalize simplicity. A simple architecture that serves the domain is better than an overengineered one.</instruction>
    <instruction>Do not reward complexity. More abstractions is not better unless they solve real problems.</instruction>
    <instruction>Score based on what you observe in the code, not on what the documentation claims.</instruction>
    <instruction>A solo developer building this in two weeks should be scored against "what's reasonable for that context," not against what a 50-person team would produce.</instruction>
  </anti-bias-instructions>
</evaluation-panel>
]]>