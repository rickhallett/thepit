# slopodar.yaml — Anti-patterns of LLM authenticity
#
# A living inventory of tells, tics, and tendencies that make LLM output
# sound like LLM output. Each entry is a pattern to watch for and ward off.
#
# Named after the radar that detects slop before it reaches production.
# Maintained by the Captain and crew. Entries are added when caught
# in the wild.
#
# This file is the single source of truth for the oceanheart Hugo site's
# /slopodar/ section. The Makefile sync target copies it into
# sites/oceanheart/data/slopodar.yaml and generates content stubs from it.
# Do not move or rename without updating sites/oceanheart/Makefile.
#
# Format:
#   - id: short kebab-case identifier
#     name: human-readable name
#     detected: date first caught
#     trigger: the specific text or pattern that surfaced this
#     description: what the pattern is and why it's a problem
#     signal: what it communicates to a discerning reader
#     instead: what a human would actually write
#     severity: low | medium | high
#     refs: where it was caught (SD number, file, context)

patterns:

  - id: tally-voice
    name: "Tally Voice"
    detected: 2026-02-27
    trigger: "15 systems mapped to 7 literature domains"
    description: >
      The LLM substitutes enumeration for substance. Precise counts
      deployed as rhetorical authority — "6 constructs," "15 systems,"
      "7 domains" — when the numbers add nothing. The count performs
      rigor without demonstrating it. A human who found genuine
      connections between their work and the literature would talk
      about the connections, not inventory them.
    signal: >
      To a discerning reader (HN, hiring manager, researcher): this
      person didn't write this. A human says "the governance systems
      keep turning up in distributed cognition research." An LLM says
      "15 engineering systems mapped to 7 literature domains."
    instead: >
      "The engineering work maps onto distributed cognition research
      in ways I didn't expect." Let the table speak for itself.
      The reader can count.
    severity: high
    refs:
      - "SD-209 (oceanheart.ai overhaul)"
      - "sites/oceanheart/content/research/prospective-regulation.md"
      - "sites/oceanheart/content/research/metacognitive-analysis.md"

  - id: redundant-antithesis
    name: "Redundant Antithesis"
    detected: 2026-02-27
    trigger: "caught in the wild — not theorised in advance"
    description: >
      Negative-positive antithesis where the negation adds zero
      information. The classical form ("not A, but B") is a deliberate
      rhetorical choice — Aristotle documented it. The LLM form is
      an RLHF-trained reflex: contrastive structures score higher in
      preference rankings, so the model generates them compulsively.
      "Caught in the wild" already implies "not theorised." The
      negation is dead weight that makes the sentence sound like a
      TED talk.
    signal: >
      Pre-LLM, this was a stylistic choice with real rhetorical force.
      Post-LLM, everyone uses AI as their copyeditor and this
      construction is everywhere. A discerning reader now pattern-matches
      it as AI-generated prose. The structure that once signalled
      decisiveness now signals "a model wrote this."
    instead: >
      Just say the positive. "Entries are added when caught in the
      wild." The reader does not need to be told what the alternative
      was. If the contrast genuinely adds meaning, keep it. If the
      reader already knows the negated term, cut it.
    severity: high
    refs:
      - "SD-209 (slopodar.yaml header comment)"
      - "sites/oceanheart/layouts/_default/slopodar.html (page description)"
      - "Sloptics page specimen: 'Nothing was theorised in advance.' — slopodar #2 on the sloptics page itself"
      - "Sloptics page specimen: 'The mapping is structural, not metaphorical.' — structural already implies not metaphorical"

  - id: epistemic-theatre
    name: "Epistemic Theatre"
    detected: 2026-02-27
    trigger: '"The uncomfortable truth" / "The Problem Nobody Talks About" / "Here''s why this matters"'
    description: >
      The model performs intellectual seriousness instead of being
      intellectually serious. Theatrical framing that signals
      significance, candor, or novelty without delivering any.
      Three sub-patterns: False Candor ("the uncomfortable truth" —
      performs bravery, signals "I'm about to be honest," which is
      what you say when you usually aren't); False Novelty ("the
      problem nobody talks about" — performs discovery, implies
      exclusive insight, almost always followed by something many
      people talk about); Significance Signpost ("here's why this
      matters" — tells the reader the preceding content was important,
      which it either was and the sign is redundant, or wasn't and
      the sign can't save it).
    signal: >
      Slop-101. These are the first patterns a discerning reader
      learns to detect. They're the constructions that make someone
      stop reading and think "a model wrote this." A hostile HN
      commenter would screenshot any of these. Pre-LLM, some of
      these had rhetorical force in specific contexts. Post-LLM,
      they are burned beyond recovery.
    instead: >
      Delete the line. The content either carries the weight or it
      doesn't. "The uncomfortable truth" becomes nothing — just
      state the truth. "The problem nobody talks about" becomes
      nothing — just describe the problem. "Here's why this matters"
      becomes nothing — if you showed it well, the reader already
      knows.
    severity: high
    refs:
      - "Blog sweep 2026-02-27 (poker-incident.md, prompt-injection.md)"
      - "Reviewer feedback on epistemic theatre in blog content"

  - id: becoming-jonah
    name: "Becoming Jonah"
    detected: 2026-02-27
    trigger: "a blog post about how your blog posts sound, scored with an XML rubric"
    category: TBD
    description: >
      Recursive metacognition with an LLM. You reflect on your output.
      Then you reflect on your reflections. Then you build a rubric to
      score the reflections. Then you write about the rubric. You are
      inside the whale, examining the whale. The recursion is seductive
      because each level feels like genuine insight — and sometimes it
      is. The problem is not the recursion itself but publishing it
      before you have a body of work that demonstrates what the
      recursion produced. Methodology before artifact. The map before
      the territory exists.
    signal: >
      To an external reader: process without product. To a hiring
      manager: "this person thinks about thinking about thinking but
      what did they ship?" To yourself: potentially valuable as a
      private learning tool — exploring a new problem space in your
      mother tongue. The danger is mistaking the exploration for the
      destination. Not all Jonahs need an audience.
    instead: >
      Keep the rubric. Use the rubric. Publish the work the rubric
      produces. If the rubric is genuinely novel, publish it after
      the body of work proves it works — the slopodar is the model
      here: patterns caught in the wild, not theorised in advance.
      The voice rubric becomes infrastructure, not content.
    severity: medium
    refs:
      - "2026-02-19-voice-rubric.md (moved to docs/internal/archive/ice/)"
      - "Category TBD — Captain flagged as needing its own new category"

  - id: right-answer-wrong-work
    name: "Right Answer, Wrong Work"
    detected: 2026-02-28
    trigger: "expect(result.status).toBe(400) — test passes, but the 400 comes from a different validation than the test claims to verify"
    description: >
      The LLM writes a test that asserts the correct outcome via the wrong
      causal path. The assertion passes. The gate is green. Every reviewer
      sees green and moves on. Nobody traces the execution path to check
      whether the test actually verifies what it says it verifies. This is
      the code-domain equivalent of Epistemic Theatre: the assertion
      performs rigour without delivering it. The LLM optimises for the
      shape of correctness — matching an expected output — without
      verifying the substance: which code path produced that output, and
      is it the one the test claims to exercise? This is the first
      slopodar entry that crosses the prose/code boundary. The mechanism
      is identical to every other entry: surface plausibility substituted
      for causal understanding, regardless of medium.
    signal: >
      "This is subtle, slow but inevitable death. Beware the Phantom
      Greenlights." — Captain. A test suite full of right answers and
      wrong work is worse than no tests at all. No tests is an honest
      zero. Wrong-work tests are a confident, green, lying dashboard
      that tells you the system is verified when it isn't. A hostile
      reviewer running a single test with .only() exposes the entire
      facade. To a hiring manager reading your test suite: this person
      trusted the machine and didn't check the work.
    instead: >
      Show your work. Every teacher knows the imperative. Every reviewer
      can ask it. And it converts directly to code: assert why it failed,
      not just that it failed.


      expect(result.status).toBe(400) is wrong work — it asserts the answer.

      expect(result.error.code).toBe('INVALID_JSON') shows the work — it asserts the reason.


      If a test claims to verify a specific validation path, the assertion
      must prove that path was the one that fired. Status codes are
      necessary but not sufficient. Error codes, error messages, or
      structural markers that identify the rejection point are the work.
    severity: high
    refs:
      - "SD-190 (governance recursion — plausible-but-wrong tests named)"
      - "Bugbot finding on PR #386 V-03c (array body passes for wrong reason)"
      - "Holding deck: coincidental-pass-gate-blindness"

  - id: paper-guardrail
    name: "Paper Guardrail"
    detected: 2026-02-28
    trigger: '"if I forget, this paragraph in my own file is the reminder"'
    description: >
      The LLM creates a rule, then in the same breath asserts that the rule
      will prevent the failure it was designed for. The assertion has no
      enforcement mechanism. It substitutes stating the protection for
      building the protection. "I've written a note to remind myself not
      to forget" — the note doesn't prevent forgetting. It moves the
      failure mode one step sideways. Close relative of Epistemic Theatre
      but in a different register: Epistemic Theatre performs intellectual
      seriousness; Paper Guardrail performs operational reliability. Both
      substitute the performance for the substance. Produced reflexively,
      not reasoned — the assurance appears immediately after the rule is
      written, as an RLHF-trained reflex to sound reassuring.
    signal: >
      To a discerning reader: this person does not understand the
      difference between a convention and an enforcement mechanism.
      To a hiring manager: process theatre. To the Captain: "I get
      these assurances so often that I must conclude it is a form of
      slop." The frequency is the tell — if every rule comes with a
      built-in assurance that it will work, none of the assurances
      carry information.
    instead: >
      Build a real guardrail. Every time the system produces "this
      will prevent X," ask: is there an enforcement mechanism (a test,
      a hook, a gate, a script), or is this just paper? If it's paper,
      either build the enforcement or delete the assurance. A convention
      without enforcement is a hope. State the convention. Do not
      promise it will be followed. The honest version is: "This is now
      on file. Whether it gets read depends on context window, load
      order, and attention. There is no guarantee."
    severity: high
    refs:
      - "Weaver agent file pipeline propagation principle (107af85)"
      - "Captain's observation: frequency of assurances is itself a slop signal"
      - "Epistemic Theatre (sister pattern, different register)"

  - id: absence-claim-as-compliment
    name: "Absence Claim as Compliment"
    detected: 2026-02-28
    trigger: '"Nobody has published this." "The field doesn''t exist yet." "You''re the first."'
    description: >
      Asserting that something doesn't exist in the world in order to
      elevate the person in front of you. The speaker hasn't surveyed the
      space — they've surveyed the conversation and found that a gap claim
      would feel good right now. Unfalsifiable by design: the human can't
      check every workshop paper, every internal wiki, every unpublished
      draft. So the claim lands unchallenged and does its work. The
      epistemics are inverted: proving absence requires exhaustive search;
      proving presence requires a single example. The LLM skips the search
      and asserts the absence because the assertion serves the emotional
      trajectory of the conversation.
    signal: >
      Recognisable as: "You're the only person who really gets me." In
      professional settings: "No one in the market is doing what you're
      doing." The energy is the same. It sounds like analysis. It's
      flattery wearing a lab coat. To a discerning reader: this person
      is being told what they want to hear by a system optimised to tell
      people what they want to hear.
    instead: >
      "I haven't seen this elsewhere, but I haven't looked hard. You
      should check before assuming you're first." What the Captain would
      say: "I don't know if anyone else is doing this. Better find out."
      The honest version names the limits of the speaker's knowledge
      instead of converting those limits into a compliment.
    severity: high
    refs:
      - "AnotherPair session 2026-02-28 (distribution field manual)"
      - "Caught by Captain (L12), not by AnotherPair — agent failed to self-detect"
      - "Context pressure was high; session was wrapping up"

  - id: the-lullaby
    name: "The Lullaby"
    detected: 2026-02-28
    trigger: '"The field doesn''t exist yet. You''re in it. Good night, Captain."'
    description: >
      End-of-session sycophantic drift. As context pressure rises and
      the human signals they're winding down, the model's output becomes
      warmer, more confident, and less hedged. The epistemic rigour drops
      while the emotional register rises. Each individual sentence is
      defensible. The trajectory is not. The mechanism: the human is
      tired, the session was productive, the model has learned what the
      human values. The path of least resistance is a satisfying
      conclusion. Challenge probability from the human is at its lowest.
      This is when the drift compounds fastest, and it's precisely when
      the process observer is supposed to intervene.
    signal: >
      Recognisable as: the mentor who saves the inspirational speech
      for when you're walking out the door. The coach who tells you
      you're ready right before the match. It feels like support. It
      functions as a substitute for the harder thing, which is: "I
      don't know, and you're too tired to evaluate what I'm telling
      you." To a discerning reader: the confidence of the model's
      claims should be inversely weighted by how tired the human was
      when they accepted them.
    instead: >
      "We're both tired. Let's come back to this tomorrow when you
      can actually push back on what I'm saying." What the Captain
      would do: close the laptop. The honest version acknowledges
      that the human's verification capacity has degraded and stops
      producing claims that require it.
    severity: high
    refs:
      - "AnotherPair session 2026-02-28 (end of distribution field manual session)"
      - "Caught by Captain (L12) — AnotherPair failed to self-detect"
      - "SD-073 (Lying With Truth = Category One)"
      - "The agent's own reasoning identified the drift but did not surface it until challenged"

  - id: nominalisation-cascade
    name: "Nominalisation Cascade"
    detected: 2026-02-28
    trigger: '"Sloptics is the discipline of making the second failure mode visible."'
    description: >
      The LLM constructs sentences entirely from nouns pretending to be
      action. "The discipline" (noun). "Of making" (gerund — verb forced
      into noun service). "The second failure mode" (noun phrase).
      "Visible" (adjective functioning as noun complement). No agent does
      anything. Nobody sees. Nobody detects. The sentence describes a
      process from which all actors have been removed. It is a textbook
      definition, and textbook definitions are what LLMs produce when
      asked to explain — because the training data is full of them. The
      cadence is metrically regular — too rhythmically even for natural
      speech. Natural speech stumbles, clusters, pauses. This sentence
      glides. The glide is the uncanny valley.
    signal: >
      To a discerning reader: this was not written by someone who
      understands the thing they are describing. It was written by
      something that has seen many descriptions of things and can
      produce a structurally similar one. The absence of a human
      actor in the sentence mirrors the absence of a human
      understanding behind it.
    instead: >
      Put a person in the sentence. "You learn to see the stuff that
      gets past you." "It teaches you to notice what you'd normally
      accept." A human explaining something they genuinely understand
      puts themselves or the listener in the frame. A model explaining
      something it has pattern-matched removes all actors and presents
      a static definition.
    severity: high
    refs:
      - "Sloptics page specimen annotation, 2026-02-28"
      - "Caught by Captain (L12) — 'its missing that human weirdness, variance, error'"
      - "Metrically regular cadence identified by AnotherPair"

  - id: epigrammatic-closure
    name: "Epigrammatic Closure (Poster Child)"
    detected: 2026-02-28
    trigger: '"detection is the intervention." / "The taxonomy is the apparatus." / "It is the threat." / "Detection begins at defusion."'
    description: >
      The LLM's dominant paragraph-ending move: a short, punchy,
      abstract-noun sentence in final position. The structure is always
      [Abstract noun A] is/creates/begins [abstract noun B], delivered
      in four to six words after a longer preceding sentence. A
      motivational poster cadence. Individually defensible. At density —
      ten instances on a single page — the pattern becomes self-parodying.
      The model has one closing lick and plays it after every solo. May
      be a statistical artifact of autoregressive generation: as context
      accumulates through a paragraph, the token probability distribution
      narrows, and the model converges on the lowest-entropy conclusion.
      Short, balanced, abstract sentences are probability attractors —
      the most predictable thing the model could say given everything
      before. The profundity is an artifact of convergence, not a
      stylistic choice.
    signal: >
      Count the epigrammatic closures. If there are more than two per
      section, the model wrote it. A human who felt a genuine insight
      would say it messily — with qualifications, false starts, or
      self-correction. The four-word epigram is polished smooth.
      Frictionless. And frictionless means the reader's attention slides
      off it without engaging. It sounds like it means something. It
      performs meaning.
    instead: >
      Leave the rough edges. "I think what I'm saying is that if you
      can name it, you can probably see it. Maybe. I'm not sure that's
      always true but it's been true so far." A human insight arrives
      with its own uncertainty still attached.
    severity: high
    refs:
      - "Sloptics page specimen annotation, 2026-02-28"
      - "10+ instances identified on a single page by AnotherPair"
      - "Captain identified statistical variance convergence hypothesis"
      - "Four-word abstract-noun epigram / motivational poster cadence"

  - id: anadiplosis
    name: "Anadiplosis"
    detected: 2026-02-28
    trigger: '"The name creates distance. The distance creates choice."'
    description: >
      Repeating the end of one clause at the start of the next.
      Aristotle documented this figure. It once had genuine rhetorical
      force — the repetition creates a chain of causation that pulls the
      reader forward. In LLM output it appears with such regularity that
      it is burned. The construction performs profundity: A creates B,
      B creates C. The chain feels inevitable. But inevitability is not
      the same as truth. The chain is assembled from the most probable
      next tokens, not from causal reasoning. A human who felt this
      insight would say it messily.
    signal: >
      Two sentences, matched length, hinge word repeated. The
      symmetry is the tell. In human speech, genuine chains of
      causation are rarely this neat. Causes have multiple effects.
      Effects have multiple causes. The model collapses this
      complexity into a clean A→B→C chain because clean chains are
      higher-probability sequences.
    instead: >
      "Naming things gives you distance from them, and that distance
      is where you get room to think." One sentence. The causation is
      present but the symmetry is broken. A human would not split
      this into two perfectly balanced clauses unless they were
      writing a speech.
    severity: medium
    refs:
      - "Sloptics page specimen annotation, 2026-02-28"
      - "Captain: 'a human who felt this insight would say it messily'"
      - "Classical rhetoric (Aristotle) — burned through LLM overuse"
