# slopodar.yaml — Anti-patterns of LLM authenticity
#
# A living inventory of tells, tics, and tendencies that make LLM output
# sound like LLM output. Each entry is a pattern to watch for and ward off.
#
# Named after the radar that detects slop before it reaches production.
# Maintained by the Captain and crew. Entries are added when caught
# in the wild.
#
# This file is the single source of truth for the oceanheart Hugo site's
# /slopodar/ section. The Makefile sync target copies it into
# sites/oceanheart/data/slopodar.yaml and generates content stubs from it.
# Do not move or rename without updating sites/oceanheart/Makefile.
#
# Format:
#   - id: short kebab-case identifier
#     name: human-readable name
#     detected: date first caught
#     trigger: the specific text or pattern that surfaced this
#     description: what the pattern is and why it's a problem
#     signal: what it communicates to a discerning reader
#     instead: what a human would actually write
#     severity: low | medium | high
#     refs: where it was caught (SD number, file, context)

patterns:

  - id: tally-voice
    name: "Tally Voice"
    detected: 2026-02-27
    trigger: "15 systems mapped to 7 literature domains"
    description: >
      The LLM substitutes enumeration for substance. Precise counts
      deployed as rhetorical authority — "6 constructs," "15 systems,"
      "7 domains" — when the numbers add nothing. The count performs
      rigor without demonstrating it. A human who found genuine
      connections between their work and the literature would talk
      about the connections, not inventory them.
    signal: >
      To a discerning reader (HN, hiring manager, researcher): this
      person didn't write this. A human says "the governance systems
      keep turning up in distributed cognition research." An LLM says
      "15 engineering systems mapped to 7 literature domains."
    instead: >
      "The engineering work maps onto distributed cognition research
      in ways I didn't expect." Let the table speak for itself.
      The reader can count.
    severity: high
    refs:
      - "SD-209 (oceanheart.ai overhaul)"
      - "sites/oceanheart/content/research/prospective-regulation.md"
      - "sites/oceanheart/content/research/metacognitive-analysis.md"

  - id: redundant-antithesis
    name: "Redundant Antithesis"
    detected: 2026-02-27
    trigger: "caught in the wild — not theorised in advance"
    description: >
      Negative-positive antithesis where the negation adds zero
      information. The classical form ("not A, but B") is a deliberate
      rhetorical choice — Aristotle documented it. The LLM form is
      an RLHF-trained reflex: contrastive structures score higher in
      preference rankings, so the model generates them compulsively.
      "Caught in the wild" already implies "not theorised." The
      negation is dead weight that makes the sentence sound like a
      TED talk.
    signal: >
      Pre-LLM, this was a stylistic choice with real rhetorical force.
      Post-LLM, everyone uses AI as their copyeditor and this
      construction is everywhere. A discerning reader now pattern-matches
      it as AI-generated prose. The structure that once signalled
      decisiveness now signals "a model wrote this."
    instead: >
      Just say the positive. "Entries are added when caught in the
      wild." The reader does not need to be told what the alternative
      was. If the contrast genuinely adds meaning, keep it. If the
      reader already knows the negated term, cut it.
    severity: high
    refs:
      - "SD-209 (slopodar.yaml header comment)"
      - "sites/oceanheart/layouts/_default/slopodar.html (page description)"

  - id: epistemic-theatre
    name: "Epistemic Theatre"
    detected: 2026-02-27
    trigger: '"The uncomfortable truth" / "The Problem Nobody Talks About" / "Here''s why this matters"'
    description: >
      The model performs intellectual seriousness instead of being
      intellectually serious. Theatrical framing that signals
      significance, candor, or novelty without delivering any.
      Three sub-patterns: False Candor ("the uncomfortable truth" —
      performs bravery, signals "I'm about to be honest," which is
      what you say when you usually aren't); False Novelty ("the
      problem nobody talks about" — performs discovery, implies
      exclusive insight, almost always followed by something many
      people talk about); Significance Signpost ("here's why this
      matters" — tells the reader the preceding content was important,
      which it either was and the sign is redundant, or wasn't and
      the sign can't save it).
    signal: >
      Slop-101. These are the first patterns a discerning reader
      learns to detect. They're the constructions that make someone
      stop reading and think "a model wrote this." A hostile HN
      commenter would screenshot any of these. Pre-LLM, some of
      these had rhetorical force in specific contexts. Post-LLM,
      they are burned beyond recovery.
    instead: >
      Delete the line. The content either carries the weight or it
      doesn't. "The uncomfortable truth" becomes nothing — just
      state the truth. "The problem nobody talks about" becomes
      nothing — just describe the problem. "Here's why this matters"
      becomes nothing — if you showed it well, the reader already
      knows.
    severity: high
    refs:
      - "Blog sweep 2026-02-27 (poker-incident.md, prompt-injection.md)"
      - "Reviewer feedback on epistemic theatre in blog content"

  - id: becoming-jonah
    name: "Becoming Jonah"
    detected: 2026-02-27
    trigger: "a blog post about how your blog posts sound, scored with an XML rubric"
    category: TBD
    description: >
      Recursive metacognition with an LLM. You reflect on your output.
      Then you reflect on your reflections. Then you build a rubric to
      score the reflections. Then you write about the rubric. You are
      inside the whale, examining the whale. The recursion is seductive
      because each level feels like genuine insight — and sometimes it
      is. The problem is not the recursion itself but publishing it
      before you have a body of work that demonstrates what the
      recursion produced. Methodology before artifact. The map before
      the territory exists.
    signal: >
      To an external reader: process without product. To a hiring
      manager: "this person thinks about thinking about thinking but
      what did they ship?" To yourself: potentially valuable as a
      private learning tool — exploring a new problem space in your
      mother tongue. The danger is mistaking the exploration for the
      destination. Not all Jonahs need an audience.
    instead: >
      Keep the rubric. Use the rubric. Publish the work the rubric
      produces. If the rubric is genuinely novel, publish it after
      the body of work proves it works — the slopodar is the model
      here: patterns caught in the wild, not theorised in advance.
      The voice rubric becomes infrastructure, not content.
    severity: medium
    refs:
      - "2026-02-19-voice-rubric.md (moved to docs/internal/archive/ice/)"
      - "Category TBD — Captain flagged as needing its own new category"

  - id: right-answer-wrong-work
    name: "Right Answer, Wrong Work"
    detected: 2026-02-28
    trigger: "expect(result.status).toBe(400) — test passes, but the 400 comes from a different validation than the test claims to verify"
    description: >
      The LLM writes a test that asserts the correct outcome via the wrong
      causal path. The assertion passes. The gate is green. Every reviewer
      sees green and moves on. Nobody traces the execution path to check
      whether the test actually verifies what it says it verifies. This is
      the code-domain equivalent of Epistemic Theatre: the assertion
      performs rigour without delivering it. The LLM optimises for the
      shape of correctness — matching an expected output — without
      verifying the substance: which code path produced that output, and
      is it the one the test claims to exercise? This is the first
      slopodar entry that crosses the prose/code boundary. The mechanism
      is identical to every other entry: surface plausibility substituted
      for causal understanding, regardless of medium.
    signal: >
      "This is subtle, slow but inevitable death. Beware the Phantom
      Greenlights." — Captain. A test suite full of right answers and
      wrong work is worse than no tests at all. No tests is an honest
      zero. Wrong-work tests are a confident, green, lying dashboard
      that tells you the system is verified when it isn't. A hostile
      reviewer running a single test with .only() exposes the entire
      facade. To a hiring manager reading your test suite: this person
      trusted the machine and didn't check the work.
    instead: >
      Show your work. Every teacher knows the imperative. Every reviewer
      can ask it. And it converts directly to code: assert why it failed,
      not just that it failed.


      expect(result.status).toBe(400) is wrong work — it asserts the answer.

      expect(result.error.code).toBe('INVALID_JSON') shows the work — it asserts the reason.


      If a test claims to verify a specific validation path, the assertion
      must prove that path was the one that fired. Status codes are
      necessary but not sufficient. Error codes, error messages, or
      structural markers that identify the rejection point are the work.
    severity: high
    refs:
      - "SD-190 (governance recursion — plausible-but-wrong tests named)"
      - "Bugbot finding on PR #386 V-03c (array body passes for wrong reason)"
      - "Holding deck: coincidental-pass-gate-blindness"
